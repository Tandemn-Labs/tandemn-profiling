[
  {
    "tp": 4,
    "pp": 4,
    "max_input_length": 2048,
    "max_output_length": 512,
    "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
    "status": "error",
    "error": "\u001b[36mray::RayWorkerWrapper.execute_method()\u001b[39m (pid=4545, ip=172.31.26.111, actor_id=3c7f9d5f5403807167d7a4b101000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x7c55641a2d80>)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/sky_workdir/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py\", line 582, in execute_method\n    raise e\n  File \"/home/ubuntu/sky_workdir/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py\", line 573, in execute_method\n    return run_method(target, method, args, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/sky_workdir/.venv/lib/python3.12/site-packages/vllm/utils.py\", line 2196, in run_method\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/sky_workdir/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/sky_workdir/.venv/lib/python3.12/site-packages/vllm/worker/worker.py\", line 229, in determine_num_available_blocks\n    self.model_runner.profile_run()\n  File \"/home/ubuntu/sky_workdir/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/sky_workdir/.venv/lib/python3.12/site-packages/vllm/worker/model_runner.py\", line 1235, in profile_run\n    self._dummy_run(max_num_batched_tokens, max_num_seqs)\n  File \"/home/ubuntu/sky_workdir/.venv/lib/python3.12/site-packages/vllm/worker/model_runner.py\", line 1346, in _dummy_run\n    self.execute_model(model_input, kv_caches, intermediate_tensors)\n  File \"/home/ubuntu/sky_workdir/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/sky_workdir/.venv/lib/python3.12/site-packages/vllm/worker/model_runner.py\", line 1772, in execute_model\n    logits = self.model.compute_logits(hidden_or_intermediate_states,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/sky_workdir/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 557, in compute_logits\n    logits = self.logits_processor(self.lm_head, hidden_states,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/sky_workdir/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/sky_workdir/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/sky_workdir/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/logits_processor.py\", line 74, in forward\n    logits = self._get_logits(hidden_states, lm_head, embedding_bias)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/sky_workdir/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/logits_processor.py\", line 116, in _get_logits\n    logits = self._gather_logits(logits)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/sky_workdir/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/logits_processor.py\", line 101, in _gather_logits\n    logits = tensor_model_parallel_gather(logits)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/sky_workdir/.venv/lib/python3.12/site-packages/vllm/distributed/communication_op.py\", line 26, in tensor_model_parallel_gather\n    return get_tp_group().gather(input_, dst, dim)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/sky_workdir/.venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py\", line 338, in gather\n    return self.device_communicator.gather(input_, dst, dim)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/sky_workdir/.venv/lib/python3.12/site-packages/vllm/distributed/device_communicators/base_device_communicator.py\", line 86, in gather\n    torch.distributed.gather(input_,\n  File \"/home/ubuntu/sky_workdir/.venv/lib/python3.12/site-packages/torch/distributed/c10d_logger.py\", line 83, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/sky_workdir/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py\", line 3620, in gather\n    work = group.gather(output_tensors, input_tensors, opts)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Inplace update to inference tensor outside InferenceMode is not allowed.You can make a clone to get a normal tensor before doing inplace update.See https://github.com/pytorch/rfcs/pull/17 for more details.",
    "instance_type": "4x g6e.12xlarge",
    "price_per_hour": 18.72,
    "num_nodes": 4,
    "gpus_per_node": 4,
    "total_gpus": 16,
    "llm_model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
    "llm_tensor_parallel_size": 4,
    "llm_pipeline_parallel_size": 4,
    "llm_max_model_len": 2559,
    "llm_trust_remote_code": true,
    "llm_distributed_executor_backend": "ray",
    "llm_gpu_memory_utilization": 0.85,
    "llm_enforce_eager": true,
    "llm_enable_chunked_prefill": false,
    "llm_enable_prefix_caching": false,
    "llm_kv_transfer_config": null,
    "sampling_temperature": 0.8,
    "sampling_max_tokens": 512,
    "sampling_min_tokens": 512,
    "sampling_ignore_eos": true,
    "benchmark_num_samples": 30,
    "benchmark_dataset": "emozilla/pg19-test",
    "benchmark_prompt_prefix": "Please summarize the following text: ",
    "benchmark_warmup_samples": 5,
    "gpu_monitor_sample_interval": 0.5,
    "gpu_monitor_type": "distributed",
    "scheduler_monitor_sample_interval": 0.25
  }
]