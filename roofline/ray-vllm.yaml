name: vllm-ray-ec2

resources:
  cloud: aws
  # Pick one:
  # instance_type: p3.2xlarge
  accelerators: L40S:1
  # Or CPU-only (vLLM wonâ€™t be meaningful without GPU):
  use_spot: false
  # cpus: 16+
  # Open ports only if you want external access (dashboard / vLLM server).
  ports:
    - 6379   # Ray head port (user Ray)
    - 8265   # Ray dashboard

workdir: . 
# this will sync the current directory 

num_nodes: 2


setup: |
  set -euxo pipefail

  python3 -m pip install -U pip
  python3 -m pip install -U uv

  uv venv --python 3.12 --seed
  source .venv/bin/activate

  # Install vLLM (pin as desired)
  uv pip install "vllm" "datasets"

run: |
  set -euxo pipefail
  source .venv/bin/activate

  # Ensure the Ray template uses *your venv ray*, not system ray.
  export RAY_CMD="uv run ray"

  # Optional: expose dashboard publicly
  export RAY_DASHBOARD_HOST=0.0.0.0
  export RAY_HEAD_PORT=6379
  export RAY_DASHBOARD_PORT=8265
  export RAY_ADDRESS="127.0.0.1:6379"


  # Start head+workers across all nodes
  ~/sky_templates/ray/start_cluster


  if [ "${SKYPILOT_NODE_RANK}" = "0" ]; then
    echo "Ray head should be up at: 127.0.0.1:${RAY_HEAD_PORT}"
    ray status --address="127.0.0.1:${RAY_HEAD_PORT}" || true
    
    # Optional: Auto-run autolauncher.py if it exists
    if [ -f "autolauncher.py" ]; then
      echo ""
      echo "========================================"
      echo "ðŸš€ Launching autolauncher.py on head node"
      echo "========================================"
      python demo_launch.py
      echo ""
      echo "âœ… autolauncher.py completed!"
    fi
  fi